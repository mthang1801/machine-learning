{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The tf.data.Dataset \n",
    "This API supports writing descriptive and efficient input pipelines. Dataset usage follows a common pattern:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import tensorflow as tf\n",
    "from tensorflow.data import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source dataset : \n",
    "The simplest way to create a dataset is to create it from a python list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0, shape=(), dtype=int64)\n",
      "tf.Tensor(1, shape=(), dtype=int64)\n",
      "tf.Tensor(2, shape=(), dtype=int64)\n",
      "tf.Tensor(3, shape=(), dtype=int64)\n",
      "tf.Tensor(4, shape=(), dtype=int64)\n",
      "tf.Tensor(5, shape=(), dtype=int64)\n",
      "tf.Tensor(6, shape=(), dtype=int64)\n",
      "tf.Tensor(7, shape=(), dtype=int64)\n",
      "tf.Tensor(8, shape=(), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(np.arange(9))\n",
    "for i in dataset : \n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0, 3.0]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(Dataset.range(5).as_numpy_iterator())\n",
    "\n",
    "list(Dataset.range(2, 5).as_numpy_iterator())\n",
    "\n",
    "list(Dataset.range(1, 5, 2).as_numpy_iterator())\n",
    "\n",
    "list(Dataset.range(1, 5, -2).as_numpy_iterator())\n",
    "\n",
    "list(Dataset.range(5, 1).as_numpy_iterator())\n",
    "\n",
    "list(Dataset.range(5, 1, -2).as_numpy_iterator())\n",
    "\n",
    "list(Dataset.range(2, 5, output_type=tf.int32).as_numpy_iterator())\n",
    "\n",
    "list(Dataset.range(1, 5, 2, output_type=tf.float32).as_numpy_iterator())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### apply"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applies a transformation function to this dataset.\n",
    "`apply` enables chaining of custom `Dataset` transformations, which are represented as functions that take one Dataset argument and return a transformed `Dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.range(100)\n",
    "def dataset_fn(ds) : \n",
    "    return ds.filter(lambda x : x < 5 )\n",
    "dataset = dataset.apply(dataset_fn)\n",
    "list(dataset.as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### as_numpy_iterator\n",
    "\n",
    "Returns an iterator which converts all elements of the dataset to numpy.\n",
    "Use `as_numpy_iterator` to inspect the content of your dataset. To see element shapes and types, print dataset elements directly instead of using `as_numpy_iterator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0, shape=(), dtype=int64)\n",
      "tf.Tensor(1, shape=(), dtype=int64)\n",
      "tf.Tensor(2, shape=(), dtype=int64)\n",
      "tf.Tensor(3, shape=(), dtype=int64)\n",
      "tf.Tensor(4, shape=(), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(np.arange(5))\n",
    "# if we don't use as_numpu_iterator\n",
    "for element in dataset:\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "# use iterator\n",
    "for element in dataset.as_numpy_iterator() :\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(dataset.as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`as_numpy_iterator()` will preserve the nested structure of dataset elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'a': (1, 3), 'b': 5}, {'a': (2, 4), 'b': 6}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices({\"a\" : ([1,2],[3,4]), \"b\": [5,6]})\n",
    "list(dataset.as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### batch\n",
    "batch(batch_size, drop_remainder=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combines consecutive elements of this dataset into batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0, 1, 2, 3]),\n",
       " array([4, 5, 6, 7]),\n",
       " array([ 8,  9, 10, 11]),\n",
       " array([12, 13, 14, 15]),\n",
       " array([16, 17, 18, 19])]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.range(20)\n",
    "dataset = dataset.batch(4, drop_remainder=True) \n",
    "list(dataset.as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3]\n",
      "[4 5 6 7]\n",
      "[ 8  9 10 11]\n",
      "[12 13 14 15]\n",
      "[16 17 18 19]\n"
     ]
    }
   ],
   "source": [
    "for element in dataset.as_numpy_iterator(): \n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The components of the resulting element will have an additional outer dimension, which will be `batch_size` (or `N % batch_size` for the last element if batch_size does not divide the number of input elements `N` evenly and drop_remainder is `False`). If your program depends on the batches having the same outer dimension, you should set the drop_remainder argument to True to prevent the smaller batch from being produced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### concatenate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creates a Dataset by concatenating the given dataset with this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4]\n",
      "[4, 5, 6, 7]\n",
      "[1, 2, 3, 4, 4, 5, 6, 7]\n"
     ]
    }
   ],
   "source": [
    "a = tf.data.Dataset.range(1,5) # => [1,2,3,4]\n",
    "b = tf.data.Dataset.range(4,8) # => [4,5,6,7]\n",
    "ds = a.concatenate(b)\n",
    "print(list(a.as_numpy_iterator()))\n",
    "print(list(b.as_numpy_iterator()))\n",
    "print(list(ds.as_numpy_iterator()))\n",
    "# The input dataset and dataset to be concatenated should have the same\n",
    "# nested structures and output types.\n",
    "c = list(zip(a,b))\n",
    "c = tf.data.Dataset.zip((a,b))\n",
    "#a.concatenate(c) # Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### enumerate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 10), (3, 11), (4, 12), (5, 13), (6, 14)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.range(10,15)\n",
    "dataset = dataset.enumerate(start=2)\n",
    "list(dataset.as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, array([7, 8], dtype=int32)), (1, array([ 9, 10], dtype=int32))]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The nested structure of the input dataset determines the structure of elements in the resulting dataset.\n",
    "dataset = tf.data.Dataset.from_tensor_slices([(7, 8), (9, 10)])\n",
    "dataset = dataset.enumerate()\n",
    "list(dataset.as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 2, 4, 6, 8, 10, 12, 14, 16, 18]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.range(20)\n",
    "dataset = dataset.filter(lambda x : x % 2 == 0)\n",
    "list(dataset.as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### flat_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maps `map_func` across this dataset and flattens the result.\n",
    "Use `flat_map` if you want to make sure that the order of your dataset stays the same. For example, to flatten a dataset of batches into a dataset of their elements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7, 4, 8, 5, 7, 3, 7, 8, 5]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def flat_map_fn(x) :\n",
    "    return tf.data.Dataset.from_tensor_slices(x)\n",
    "np.random.seed(42)\n",
    "dataset = tf.data.Dataset.from_tensor_slices(np.random.randint(1,10,(3,3)))\n",
    "dataset = dataset.flat_map(flat_map_fn)\n",
    "list(dataset.as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### from_tensor_slices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creates a `Dataset` whose elements are slices of the given tensors.\n",
    "\n",
    "The given tensors are sliced along their first dimension. This operation preserves the structure of the input tensors, removing the first dimension of each tensor and using it as the dataset dimension. All input tensors must have the same size in their first dimensions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Slicing a 1D tensor produces scalar tensor elements.\n",
    "dataset = tf.data.Dataset.from_tensor_slices([1,2,3,4])\n",
    "list(dataset.as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([1, 2, 3], dtype=int32), array([4, 5, 6], dtype=int32)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Slicing a 2D tensor produces 1D tensor elements.\n",
    "dataset = tf.data.Dataset.from_tensor_slices([[1,2,3],[4,5,6]])\n",
    "list(dataset.as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 3, 5), (2, 4, 6)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Slicing a tuple of 1D tensors produces tuple elements containing scalar tensors.\n",
    "dataset = tf.data.Dataset.from_tensor_slices(([1,2],[3,4],[5,6]))\n",
    "list(dataset.as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'a': 1, 'b': 3}, {'a': 2, 'b': 4}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dictionary structure is also preserved.\n",
    "dataset = tf.data.Dataset.from_tensor_slices({\"a\" : [1,2], \"b\" : [3,4]})\n",
    "list(dataset.as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1 3]\n",
      " [2 1]\n",
      " [3 3]], shape=(3, 2), dtype=int32)\n",
      "tf.Tensor([b'A' b'B' b'A'], shape=(3,), dtype=string)\n",
      "(array([[1, 3],\n",
      "       [2, 3]], dtype=int32), array([[b'A'],\n",
      "       [b'A']], dtype=object))\n",
      "(array([[2, 1],\n",
      "       [1, 2]], dtype=int32), array([[b'B'],\n",
      "       [b'B']], dtype=object))\n",
      "(array([[3, 3],\n",
      "       [3, 2]], dtype=int32), array([[b'A'],\n",
      "       [b'B']], dtype=object))\n"
     ]
    }
   ],
   "source": [
    "# Two tensors can be combined into one Dataset object.\n",
    "features = tf.constant([[1,3], [2,1],[3,3]]) # ==> 3x2 tensor\n",
    "print(features)\n",
    "labels = tf.constant([\"A\",\"B\",\"A\"]) # ==> 3x1 tensor\n",
    "print(labels)\n",
    "dataset = tf.data.Dataset.from_tensor_slices((features,labels))\n",
    "list(dataset.as_numpy_iterator())\n",
    "# Both the features and the labels tensors can be converted\n",
    "# to a Dataset object separately and combined after.\n",
    "features_dataset = tf.data.Dataset.from_tensor_slices(features)\n",
    "labels_dataset= tf.data.Dataset.from_tensor_slices(labels)\n",
    "dataset = tf.data.Dataset.zip((features_dataset, labels_dataset))\n",
    "list(dataset.as_numpy_iterator())\n",
    "# A batched feature and label set can be converted to a Dataset\n",
    "batched_features = tf.constant([[[1, 3], [2, 3]],\n",
    "                                [[2, 1], [1, 2]],\n",
    "                                [[3, 3], [3, 2]]], shape=(3,2,2) )\n",
    "batched_labels = tf.constant([['A', 'A'],\n",
    "                              ['B', 'B'],\n",
    "                              ['A', 'B']], shape=(3,2,1))\n",
    "dataset = tf.data.Dataset.from_tensor_slices((batched_features,batched_labels))\n",
    "for element in dataset.as_numpy_iterator():\n",
    "      print(element)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### map\n",
    "\n",
    "`Maps` map_func across the elements of this dataset.\n",
    "\n",
    "This transformation applies `map_func` to each element of this dataset, and returns a new dataset containing the transformed elements, in the same order as they appeared in the input. map_func can be used to change both the values and the structure of a dataset's elements. For example, adding 1 to each element, or projecting a subset of element components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 10, 15, 20, 25, 30]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.range(1,7)\n",
    "dataset = dataset.map(lambda x : x * 5 )\n",
    "list(dataset.as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input signature of `map_func` is determined by the structure of each element in this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, b'foo'), (2, b'bar'), (3, b'baz')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Each element is a tuple containing two `tf.Tensor` objects.\n",
    "elements = [(1, \"foo\"), (2, \"bar\"), (3, \"baz\")]\n",
    "dataset = tf.data.Dataset.from_generator(lambda : elements, (tf.int32, tf.string))\n",
    "print(list(dataset.as_numpy_iterator()))\n",
    "# `map_func` takes two arguments of type `tf.Tensor`. This function\n",
    "# projects out just the first component\n",
    "result = dataset.map(lambda x_int, y_int : x_int)\n",
    "list(result.as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reduce\n",
    "\n",
    "`reduce(initial_state, reduce_func)`\n",
    "\n",
    "Reduces the input dataset to a single element\n",
    "\n",
    "The transformation calls `reduce_func` successively on every element of the input dataset until the dataset is exhausted, aggregating information in its internal state. The `initial_state` argument is used for the initial state and the final state is returned as the result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(tf.data.Dataset.range(1,6).reduce(np.int64(0), lambda x, _ : x + 1 ).numpy())\n",
    "print(tf.data.Dataset.range(1,6).reduce(np.int64(0), lambda x, y : x + y).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### repeat\n",
    "\n",
    "`repeat(count=None)`\n",
    "\n",
    "Repeats this dataset so each original value is seen count times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 1, 2, 3, 1, 2, 3]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\n",
    "dataset = dataset.repeat(count=3)\n",
    "list(dataset.as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### shuffle\n",
    "\n",
    "`shuffle(buffer_size, seed=None, reshuffle_each_iteration=None)`\n",
    "\n",
    "Randomly shuffles the elements of this dataset.\n",
    "\n",
    "This dataset fills a buffer with `buffer_size` elements, then randomly samples elements from this buffer, replacing the selected elements with new elements. For perfect shuffling, a buffer size greater than or equal to the full size of the dataset is required.\n",
    "\n",
    "For instance, if your dataset contains 10,000 elements but buffer_size is set to 1,000, then shuffle will initially select a random element from only the first 1,000 elements in the buffer. Once an element is selected, its space in the buffer is replaced by the next (i.e. 1,001-st) element, maintaining the 1,000 element buffer.\n",
    "\n",
    "reshuffle_each_iteration controls whether the shuffle order should be different for each epoch. In TF 1.X, the idiomatic way to create epochs was through the repeat transformation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 0, 5, 1, 4]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.range(6)\n",
    "dataset = dataset.shuffle(5,reshuffle_each_iteration=True)\n",
    "list(dataset.as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### skip \n",
    "\n",
    "`skip(count)`\n",
    "\n",
    "Creates a `Dataset` that skips `count` elements from this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.range(10)\n",
    "dataset = dataset.skip(3)\n",
    "list(dataset.as_numpy_iterator())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### take\n",
    "\n",
    "`take(count)`\n",
    "\n",
    "Creates a Dataset with at most `count` elements from this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.range(10)\n",
    "dataset = dataset.take(3)\n",
    "list(dataset.as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### unbatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([0, 1, 2]), array([3, 4, 5]), array([6, 7, 8]), array([9])]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "dataset = tf.data.Dataset.range(10)\n",
    "dataset = dataset.batch(batch_size=3)\n",
    "print(list(dataset.as_numpy_iterator()))\n",
    "dataset = dataset.unbatch()\n",
    "print(list(dataset.as_numpy_iterator()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### zip\n",
    "`@staticmethod\n",
    "zip(datasets)`\n",
    "\n",
    "Creates a `Dataset` by zipping together the given datasets.\n",
    "\n",
    "This method has similar semantics to the built-in `zip()` function in Python, with the main difference being that the `datasets` argument can be an arbitrary nested structure of `Dataset` objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 4), (2, 5), (3, 6)]\n",
      "[(4, 1), (5, 2), (6, 3)]\n",
      "[array([7, 8]), array([ 9, 10]), array([11, 12])]\n",
      "(1, 4, array([7, 8]))\n",
      "(2, 5, array([ 9, 10]))\n",
      "(3, 6, array([11, 12]))\n"
     ]
    }
   ],
   "source": [
    "# The nested structure of the `datasets` argument determines the\n",
    "# structure of elements in the resulting dataset.\n",
    "a = tf.data.Dataset.range(1,4)\n",
    "b = tf.data.Dataset.range(4,7)\n",
    "ds = tf.data.Dataset.zip((a,b))\n",
    "print(list(ds.as_numpy_iterator()))\n",
    "ds = tf.data.Dataset.zip((b,a))\n",
    "print(list(ds.as_numpy_iterator()))\n",
    "\n",
    "# The `datasets` argument may contain an arbitrary number of datasets.\n",
    "c = tf.data.Dataset.range(7, 13).batch(2) \n",
    "print(list(c.as_numpy_iterator()))\n",
    "ds = tf.data.Dataset.zip((a,b,c))\n",
    "for ele in ds.as_numpy_iterator() : \n",
    "    print(ele)\n",
    "    \n",
    "# The number of elements in the resulting dataset is the same as the size of the smallest dataset in `datasets`\n",
    "d = tf.data.Dataset.range(13, 15)\n",
    "ds = df.ta"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
